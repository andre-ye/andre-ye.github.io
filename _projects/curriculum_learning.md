---
layout: page
title: Transcription Curriculum Learning
description: Improving data efficiency for large-scale transcription model training
img: assets/img/curriculum.png
importance: 100
category: research
---

Deepgram uses large datasets to train even larger models. While this allows for powerful end-to-end high-capacity speech transcription systems, it also is computationally expensive and restricts the efficiency of experimental R&D. I find that intentionally restricting the dataset can significantly improve the efficiency of model training between two to five times, and particularly results in substantive absolute improvements to the diminishing-returns regime of large model training.
Read the writeup [here](https://andre-ye.github.io/assets/pdf/Curriculum_Learning_Deepgram_Final.pdf){:target="_blank"}

<iframe src="/assets/pdf/Curriculum_Learning_Deepgram_Final.pdf" width="100%" height="400" style="border:1px solid black;"></iframe>