{
    "categories": [
        {
            "title": "Position Papers",
            "description": "These papers make really interesting arguments about human-AI interaction which I find compelling or at least useful to think about.",
            "papers": [
                {
                    "title": "We Can't Understand AI Using our Existing Vocabulary",
                    "authors": "John Hewitt, Robert Geirhos, Been Kim",
                    "year": 2025,
                    "commentary": "A compelling articulation of what human-AI communication could look like. Proposes neologism learning.",
                    "link": "https://openreview.net/pdf?id=asQJx56NqB"
                },
                {
                    "title": "AI Should Not Be An Imitation Game: Centaur Evaluations",
                    "authors": "Andreas Haupt, Erik Brynjolfsson",
                    "year": 2025,
                    "commentary": "Advocates and proposes directions for systematic AI evalutions involving real human interaction.",
                    "link": "https://openreview.net/pdf?id=LkdH35003E"
                },
                {
                    "title": "AI Technologies are System Maps, and You are a Cartographer",
                    "authors": "Nicholas Vincent",
                    "year": 2023,
                    "commentary": "Explains the 'data as labor' perspective for AI via the compelling analogy of map economics.",
                    "link": "https://dataleverage.substack.com/p/ai-technologies-are-system-maps-and-you-are-a-cartographer"
                },
                {
                    "title": "Toward cultural interpretability: A linguistic anthropological framework for describing and evaluating large language models",
                    "authors": "Graham M Jones, Shai Satran, Arvind Satyanarayan",
                    "year": 2025,
                    "commentary": "Advocates for understanding LLM behavior as indicative of nuances in human social behavior.",
                    "link": "https://vis.csail.mit.edu/pubs/cultural-interpretability.pdf"
                },
                {
                    "title": "HCI for AGI",
                    "authors": "Meredith Ringel Morris",
                    "year": 2025,
                    "commentary": "Useful outline of what HCI researchers can contribute to 'AGI'. It's not obvious (and people may fear that) interaction problems will be solved by AGI. Perhaps not?",
                    "link": "https://dl.acm.org/doi/10.1145/3708815"
                },
                {
                    "title": "AI and the Demise of College Writing",
                    "authors": "Adam Walker",
                    "year": 2025,
                    "commentary": "Advocates for rhetoric over composition as the methodology for writing pedagogy in the AI era.",
                    "link": "https://www.youtube.com/watch?v=_PPx4KV8SaQ"
                }
            ]
        },

        {
            "title": "AI Tools for Human Knowledge",
            "description": "These tools leverage the properties of AI to help us know more.",
            "papers": [
                {
                    "title": "Sparse Autoencoders for Hypothesis Generation",
                    "authors": "Rajiv Movva, Kenny Peng, Nikhil Garg, Jon Kleinberg, Emma Pierson",
                    "year": 2025,
                    "commentary": "This paper uses sparse autoencoder features to identify possible hypotheses to explain relationships between text and a dependent variable.",
                    "link": "https://arxiv.org/abs/2502.04382"
                },
                {
                    "title": "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM",
                    "authors": "Michelle S. Lam, Janice Teoh, James Landay, Jeffrey Heer, Michael S. Bernstein",
                    "year": 2024,
                    "commentary": "This paper defines LLM operations for extracting concepts from large amounts of unstructured text, useful for social sciences inquiry.",
                    "link": "https://arxiv.org/abs/2404.12259"
                },
                {
                    "title": "Bridging the Human-AI Knowledge Gap: Concept Discovery and Transfer in AlphaZero",
                    "authors": "Lisa Schut, Nenad Tomasev, Tom McGrath, Demis Hassabis, Ulrich Paquet, Been Kim",
                    "year": 2023,
                    "commentary": "The authors show how machine-unique/discovered chess-playing concepts can be extracted and taught to human chess-players.",
                    "link": "https://arxiv.org/abs/2310.16410"
                }
            ]
        },

        {
            "title": "Concept-structured AI",
            "description": "These approaches systematically build human-level concepts into the way AI models are designed, so we can understand and intervene.",
            "papers": [
                {
                    "title": "Jury Learning: Integrating Dissenting Voices into Machine Learning Models",
                    "authors": "Mitchell L. Gordon, Michelle S. Lam, Joon Sung Park, Kayur Patel, Jeffrey T. Hancock, Tatsunori Hashimoto, Michael S. Bernstein",
                    "year": 2022,
                    "commentary": "By modeling individual views rather than an aggregated 'view', we can explicitly define the voices 'heard' in making a decision and consider counterfactuals.",
                    "link": "https://arxiv.org/abs/2202.02950"
                },
                {
                    "title": "Concept Bottleneck Models",
                    "authors": "Pang Wei Koh, Thao Nguyen, Yew Siang Tang, Stephen Mussmann, Emma Pierson, Been Kim, Percy Liang",
                    "year": 2020,
                    "commentary": "By learning explicitly defined concepts to bridge independent and dependent variables, we can better interpret model decisions and intervene on mistakes.",
                    "link": "https://arxiv.org/abs/2007.04612"
                },
                {
                    "title": "Explaining Datasets in Words: Statistical Models with Natural Language Parameters",
                    "authors": "Ruiqi Zhong, Heng Wang, Dan Klein, Jacob Steinhardt",
                    "year": 2024,
                    "commentary": "Language modeling in terms of natural language predicates",
                    "link": "https://arxiv.org/abs/2409.08466"
                },
                {
                    "title": "Large Concept Models: Language Modeling in a Sentence Representation Space",
                    "authors": "Lo√Øc Barrault, Paul-Ambroise Duquenne, Maha Elbayad, Artyom Kozhevnikov, Belen Alastruey, et al.",
                    "year": 2024,
                    "commentary": "Language modeling operating in embedding rather than token space.",
                    "link": "https://arxiv.org/abs/2412.08821"
                },
                {
                    "title": "Backpack Language Models",
                    "authors": "John Hewitt, John Thickstun, Christopher D. Manning, Percy Liang",
                    "year": 2023,
                    "commentary": "By creating an LM architecture in which input tokens have a direct log-linear effect on the output, we can intervene precisely on the model output.",
                    "link": "https://arxiv.org/abs/2305.16765"
                }
            ]
        },


        {
            "title": "Interpretability",
            "description": "Interesting works on understanding how complex models produce outputs and represent knowledge.",
            "papers": [
                {
                    "title": "Finding Neurons in a Haystack: Case Studies with Sparse Probing",
                    "authors": "Wes Gurnee, Neel Nanda, Matthew Pauly, Katherine Harvey, Dmitrii Troitskii, Dimitris Bertsimas",
                    "year": 2023,
                    "commentary": "By probing with sparsity constraints, we can identify not only if model activations represent some feature but whether specific neurons encode certain features.",
                    "link": "https://arxiv.org/abs/2305.01610"
                }
            ]
        },


        {
            "title": "Human-AI Interaction",
            "description": "Interesting HCI-oriented work exploring how we can interact with AI or with other humans mediated by AI.",
            "papers": [
                {
                    "title": "Embedding Comparator: Visualizing Differences in Global Structure and Local Neighborhoods via Small Multiples",
                    "authors": "Angie Boggust, Brandon Carter, Arvind Satyanarayan",
                    "year": 2022,
                    "commentary": "A very useful interface for comparing differences in embeddings with interesting applications to model training development and the social sciences.",
                    "link": "https://arxiv.org/abs/1912.04853"
                }
            ]
        }
    ]
}